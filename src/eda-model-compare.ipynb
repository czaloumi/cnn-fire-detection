{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeling_wout_time.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QY0DzAo0MOi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0du4DYv0a8Q"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDOKlGSu0baa"
      },
      "source": [
        "df = drive.CreateFile({'id': '179nL1l5rwe1505WtJwRwemCiw9BaV0S3'})\n",
        "df.GetContentFile('conditions_df.csv')\n",
        "df = pd.read_csv('conditions_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPuIA8tWBa0"
      },
      "source": [
        ",# Removing date column for simplicity - do not want to work on time series data :)\n",
        "# Dropping redundant classification columns and Stn Id to improve detecting important weather features\n",
        "to_drop = ['Date', 'Stn Id', 'Stn Name', 'CIMIS Region']\n",
        "df.drop(to_drop, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPS0_ATE40jG"
      },
      "source": [
        "print(\"Dataset has {} entries and {} features\".format(*df.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZPCcAv2Yr7C"
      },
      "source": [
        "# Important to note that \"Notes\" column is only for target columns\n",
        "# possible leakage => need to drop\n",
        "print(df[~df['Notes'].isna()]['Target'].value_counts())\n",
        "df.drop('Notes', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75UuvYqn7Db"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDno8cCHuZ5r"
      },
      "source": [
        "missing_values = 100 * df.isna().sum()/len(df)\n",
        "missing_values = missing_values.reset_index()\n",
        "missing_values.columns = ['feature', '%_nan_values']\n",
        "\n",
        "fig = px.bar(missing_values, y='%_nan_values', x='feature',\n",
        "             title='Missing Values', template='ggplot2')\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZwBgmiIwgRR"
      },
      "source": [
        "temp = df[df['Target']==1]\n",
        "missing_values = 100 * temp.isna().sum()/len(temp)\n",
        "missing_values = missing_values.reset_index()\n",
        "missing_values.columns = ['feature', '%_nan_values']\n",
        "\n",
        "fig = px.bar(missing_values, y='%_nan_values', x='feature',\n",
        "             title='Missing Values', template='ggplot2')\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM_5l_42x5Wq"
      },
      "source": [
        "# A lot of nulls in our Target column - for simplicity, filling with averages\n",
        "# Don't drop - losing a lot of info on small imbalanced target class!\n",
        "df = df.fillna(df.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyFs-UBizNoh"
      },
      "source": [
        "def plot_subgroup_hist(df, class1, class2):\n",
        "    '''\n",
        "    Displays information of 2 classes of a data set\n",
        "    \n",
        "    PARAMETERS\n",
        "    ----------\n",
        "        df: dataframe\n",
        "        class1: dataframe of 1 class\n",
        "        class2: dataframe of 2nd class\n",
        "    '''\n",
        "    dim = len(df.columns)\n",
        "    fig, axs = plt.subplots(3, int(dim/3), figsize=(12, 6))\n",
        "    for i, col_name in enumerate(class1.columns):\n",
        "        bins = np.linspace(df[col_name].min(), df[col_name].max(), 20)\n",
        "        height, binz = np.histogram(class1[col_name], bins=bins, density=True)\n",
        "        bp1 = axs[i%3][i//3].bar(bins[:-1], height, .5*(bins[1]-bins[0]),\n",
        "                     alpha=0.5, label=\"Fire\", color='r')\n",
        "        height, binz = np.histogram(class2[col_name], bins=bins, density=True)\n",
        "        bp2 = axs[i%3][i//3].bar(bins[:-1]+.5*(bins[1]-bins[0]), height,\n",
        "                     .5*(bins[1]-bins[0]), color='b', alpha=.5)\n",
        "        axs[i%3][i//3].set_title(col_name)\n",
        "        axs[i%3][i//3].legend((bp1[0], bp2[0]), (\"Fire\", \"No Fire\"), loc='best')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig, axs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNOT_vE2zUBd"
      },
      "source": [
        "df_fire = df[df['Target'] == 1]\n",
        "df_no_fire = df[df['Target'] == 0]\n",
        "\n",
        "plot_subgroup_hist(df, df_fire, df_no_fire)\n",
        "#plt.savefig('eda_histograms.png');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_iyjQ5WQk7"
      },
      "source": [
        "(df.Target.value_counts()/len(df))*100\n",
        "# quite imbalanced..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcq8tf_yaElg"
      },
      "source": [
        "# Next steps are attempting different class-balancing techniques\n",
        "# OR adjusting class_weight in model hyperparameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPLCcQ2zXD0X"
      },
      "source": [
        "y = df.pop('Target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbJIkS_1aFkE"
      },
      "source": [
        "scaled_df = StandardScaler().fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYMQWxxgkT-c"
      },
      "source": [
        "## Some fun with oop - experimenting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QF3mE8F4DV6"
      },
      "source": [
        "class Fire(object):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def split(self):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, stratify=self.y)\n",
        "\n",
        "    def predict(self, model):\n",
        "        self.model = model\n",
        "        kf = KFold(n_splits=5)\n",
        "        \n",
        "        accuracy = []\n",
        "        precision = []\n",
        "        recall = []\n",
        "        f1 = []\n",
        "\n",
        "        for train_index, test_index in kf.split(self.X_train):\n",
        "          X_train_split, X_test_split = self.X_train.iloc[train_index], self.X_train.iloc[test_index]\n",
        "          y_train_split, y_test_split = self.y_train.iloc[train_index], self.y_train.iloc[test_index]\n",
        "          self.model.fit(X_train_split, y_train_split)\n",
        "          self.pred = self.model.predict(X_test_split)\n",
        "\n",
        "          assess = lambda method, val=y_test_split, pred=self.pred: method(val, pred)\n",
        "\n",
        "          accuracy.append(assess(accuracy_score))\n",
        "          precision.append(assess(precision_score))\n",
        "          recall.append(assess(recall_score))\n",
        "          f1.append(assess(f1_score))\n",
        "        \n",
        "        return np.mean(accuracy), np.mean(precision), np.mean(recall), np.mean(f1)\n",
        "      \n",
        "    def get_rates(self):\n",
        "        self.proba = self.model.predict_proba(self.X_test)\n",
        "        self.proba = self.proba[:,1]\n",
        "        self.fpr, self.tpr, self.thresholds = roc_curve(self.y_test, self.proba)\n",
        "        self.auc = auc(self.fpr, self.tpr)\n",
        "\n",
        "        return self.fpr, self.tpr, self.auc\n",
        "\n",
        "    def cm(self):\n",
        "        return plot_confusion_matrix(self.model, self.X_test, self.y_test, cmap=plt.cm.Purples , normalize='true')\n",
        "    \n",
        "    def plot_roc(self, ax, model):\n",
        "        if model == 'knn':\n",
        "          ax.plot(self.fpr, self.tpr, color='orange', label=f'{model}: {round(self.auc, 4)}')\n",
        "          return ax\n",
        "        elif model == 'forest':\n",
        "          ax.plot(self.fpr, self.tpr, color='green', label=f'{model}: {round(self.auc, 4)}')\n",
        "          return ax\n",
        "        elif model == 'boost1':\n",
        "          ax.plot(self.fpr, self.tpr, color='red', label=f'{model}: {round(self.auc, 4)}')\n",
        "          return ax\n",
        "        else:\n",
        "          ax.plot(self.fpr, self.tpr, color='purple', label=f'{model}: {round(self.auc, 4)}')\n",
        "          return ax\n",
        "\n",
        "    def plot_importance(self):\n",
        "        return plot_importance(self.model, max_num_features=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNCVL45VvNnQ"
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "data_knn = Fire(scaled_df, y)\n",
        "data_knn.split()\n",
        "data_knn.predict(knn)\n",
        "data_knn.get_rates()\n",
        "\n",
        "data_knn.cm()\n",
        "plt.title('K Nearest Neighbors Confusion Matrix')\n",
        "plt.grid(False)\n",
        "#plt.savefig('knn_cm.jpeg')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx14w3iV8nra"
      },
      "source": [
        "forest = RandomForestClassifier(class_weight='balanced')\n",
        "\n",
        "data_forest = Fire(scaled_df, y)\n",
        "data_forest.split()\n",
        "rf_accuracy, rf_precision, rf_recall, rf_f1 = data_forest.predict(forest)\n",
        "data_forest.get_rates()\n",
        "\n",
        "data_forest.cm()\n",
        "plt.title('Random Forest Confusion Matrix')\n",
        "plt.grid(False)\n",
        "#plt.savefig('forest_cm.jpeg')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXnwI108y8z"
      },
      "source": [
        "boost = XGBClassifier(class_weight='balanced')\n",
        "\n",
        "data_boost = Fire(scaled_df, y)\n",
        "data_boost.split()\n",
        "gb_accuracy, gb_precision, gb_recall, gb_f1 = data_boost.predict(boost)\n",
        "data_boost.get_rates()\n",
        "\n",
        "data_boost.cm()\n",
        "plt.title('XGBoost Confusion Matrix')\n",
        "plt.grid(False)\n",
        "#plt.savefig('boost_cm.jpeg')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importances\n",
        "data_boost.plot_importance()\n",
        "#plt.savefig('feature_importances.jpeg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpjy0OfX6N7f"
      },
      "source": [
        "# ROC CURVE\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "data_knn.plot_roc(ax, model='knn')\n",
        "data_forest.plot_roc(ax, model='forest')\n",
        "data_boost.plot_roc(ax, model='boost')\n",
        "ax.set_title('ROC Curves')\n",
        "ax.plot([0,1], [0,1], color ='k', linestyle='--')\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.legend(loc=\"lower right\")\n",
        "#plt.savefig('untrained_model_comparison.png')\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-9pE19vkeAJ"
      },
      "source": [
        "## Fun oop over..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkcV9BODo7b9"
      },
      "source": [
        "def roc_curve(probabilities, labels):\n",
        "    '''\n",
        "    INPUT: numpy array, numpy array\n",
        "    OUTPUT: array, array, array\n",
        "\n",
        "    Take a numpy array of the predicted probabilities and a numpy array of the\n",
        "    true labels.\n",
        "    Return the True Positive Rates, False Positive Rates and Thresholds for the\n",
        "    ROC curve.\n",
        "    '''\n",
        "    # tpr = tp / tp+fn\n",
        "    # fpr = fp / fp+tn\n",
        "    \n",
        "    df = pd.DataFrame({'probabilities': probabilities, 'y': labels})\n",
        "    df.sort_values('probabilities', inplace=True)\n",
        "\n",
        "    actual_p = df.y.sum()\n",
        "    actual_n = df.shape[0] - df.y.sum()\n",
        "    \n",
        "    df['tn'] = (df.y == 0).cumsum()\n",
        "    df['fn'] = df.y.cumsum()\n",
        "    df['fp'] = actual_n - df.tn\n",
        "    df['tp'] = actual_p - df.fn\n",
        "    \n",
        "    df['fpr'] = df.fp/(df.fp + df.tn)\n",
        "    df['tpr'] = df.tp/(df.tp + df.fn)\n",
        "    df['precision'] = df.tp/(df.tp + df.fp)\n",
        "    df['F1'] = 2*((df.tp/(df.tp + df.fp)) * (df.tp/(df.tp + df.fn)))/((df.tp/(df.tp + df.fp)) + (df.tp/(df.tp + df.fn)))\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfmzxR_wo7lP"
      },
      "source": [
        "def plot_roc(ax, df):\n",
        "    ax.plot([1]+list(df.fpr), [1]+list(df.tpr), label=\"ROC\")\n",
        "    ax.plot([0,1],[0,1], 'k', label=\"random\")\n",
        "    ax.set_xlabel('FPR')\n",
        "    ax.set_ylabel('TPR')\n",
        "    ax.set_title('ROC Curve')\n",
        "    ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xewU2mago7ug"
      },
      "source": [
        "def roc_continuous_curve(probabilities, alpha, labels):\n",
        "    probabilities = np.where(probabilities > alpha, 1, 0)\n",
        "    \n",
        "    df = pd.DataFrame({'probabilities': probabilities, 'y': labels})\n",
        "    df.sort_values('probabilities', inplace=True)\n",
        "\n",
        "    actual_p = df.y.sum()\n",
        "    actual_n = df.shape[0] - df.y.sum()\n",
        "    \n",
        "    df['tn'] = (df.y == 0).cumsum()\n",
        "    df['fn'] = df.y.cumsum()\n",
        "    df['fp'] = actual_n - df.tn\n",
        "    df['tp'] = actual_p - df.fn\n",
        "    \n",
        "    df['fpr'] = df.fp/(df.fp + df.tn)\n",
        "    df['tpr'] = df.tp/(df.tp + df.fn)\n",
        "    df['precision'] = df.tp/(df.tp + df.fp)\n",
        "    df['F1'] = 2*((df.tp/(df.tp + df.fp)) * (df.tp/(df.tp + df.fn)))/((df.tp/(df.tp + df.fp)) + (df.tp/(df.tp + df.fn)))\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93ug7u-2pDCd"
      },
      "source": [
        "def model_comparison(model_list, X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    figure(figsize=(15, 10))\n",
        "\n",
        "    for m in model_list:\n",
        "        model = m # select the model\n",
        "        model.fit(X_train, y_train) # train the model\n",
        "        y_pred=model.predict(X_test) # predict the test data\n",
        "\n",
        "    # Compute False postive rate, and True positive rate\n",
        "        df = roc_curve(y_pred, y_test)\n",
        "\n",
        "    # Calculate Area under the curve to display on the plot\n",
        "        area_under_curve = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    # Plot the computed values\n",
        "        plt.plot(df.fpr, df.tpr, label=f\"{model.__class__.__name__}, {round(area_under_curve, 2)}\")\n",
        "\n",
        "    # Custom settings for the plot \n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('1-Specificity (False Positive Rate)')\n",
        "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "    plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thz_zAYWX2Z-"
      },
      "source": [
        "def gridsearch_with_output(estimator, parameter_grid, X_train, y_train):\n",
        "    '''\n",
        "    Gridsearches to hypertune estimator on training data.\n",
        "    \n",
        "    PARAMETERS\n",
        "    ----------\n",
        "        estimator: the type of model (e.g. RandomForestRegressor())\n",
        "        paramter_grid: dictionary defining the gridsearch parameters\n",
        "        X_train: 2d numpy array\n",
        "        y_train: 1d numpy array\n",
        "        \n",
        "    RETURNS\n",
        "    -------\n",
        "        best parameters and model fit with those parameters\n",
        "    '''\n",
        "    model_gridsearch = GridSearchCV(estimator,\n",
        "                                    parameter_grid,\n",
        "                                    n_jobs=-1,\n",
        "                                    verbose=10,\n",
        "                                    cv=3,\n",
        "                                    scoring='recall')\n",
        "    model_gridsearch.fit(X_train, y_train)\n",
        "    best_params = model_gridsearch.best_params_ \n",
        "    model_best = model_gridsearch.best_estimator_\n",
        "    print(\"\\nResult of gridsearch:\")\n",
        "    print(\"{0:<20s} | {1:<8s} | {2}\".format(\"Parameter\", \"Optimal\", \"Gridsearch values\"))\n",
        "    print(\"-\" * 55)\n",
        "    for param, vals in parameter_grid.items():\n",
        "        print(\"{0:<20s} | {1:<8s} | {2}\".format(str(param), \n",
        "                                                str(best_params[param]),\n",
        "                                                str(vals)))\n",
        "    return best_params, model_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReagGALPX8cS"
      },
      "source": [
        "X_train, X_test,  y_train, y_test = train_test_split(scaled_df, y, stratify=y, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCj7YdffpDNy"
      },
      "source": [
        "classification_models = [LogisticRegression(), DecisionTreeClassifier(class_weight='balanced'), RandomForestClassifier(class_weight='balanced'), GradientBoostingClassifier(class_weight='balanced')]\n",
        "\n",
        "for model in classification_models:\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(f'{model.__class__.__name__}\\n')\n",
        "  print(f'{classification_report(y_test, y_pred)}\\n')\n",
        "\n",
        "model_comparison(classification_models, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy8bsXn_tarv"
      },
      "source": [
        "parameter_grid = {\n",
        "    'max_depth': [10, 30, 50, 70],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'min_samples_leaf': [2, 4],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'n_estimators': [200, 400, 500]\n",
        "}\n",
        "\n",
        "rf_params, rf_gridsearch = gridsearch_with_output(RandomForestClassifier(class_weight='balanced'), parameter_grid, X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkrBLWw3hqJZ"
      },
      "source": [
        "# Tuned RF\n",
        "filename = 'tuned-rf.pkl'\n",
        "pickle.dump(rf_gridsearch, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}